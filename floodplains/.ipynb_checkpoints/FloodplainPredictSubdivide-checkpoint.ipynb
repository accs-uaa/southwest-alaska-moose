{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Floodplain Predict-Subdivide\n",
    "\n",
    "**Timm Nawrocki**  \n",
    "Alaska Center for Conservation Science  \n",
    "2019-04-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------------------\n",
    "# Floodplain Predict\n",
    "# Author: Timm Nawrocki, Alaska Center for Conservation Science\n",
    "# Created on: 2019-04-21\n",
    "# Usage: Must be executed as a Jupyter Notebook in an Anaconda 3 installation.\n",
    "# Description: \"Floodplain Predict-Subdivide\" applies the trained classifier to data in regular point grid format stored in csv files to create a prediction representing the distribution of floodplains. This script is meant for subwatershed grids large enough in size to require subdividing into multiple sets.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import datetime\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import time\n",
    "# Import modules for model selection, cross validation, random forest, and performance from Scikit Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Set root directory\n",
    "drive = 'E:/'\n",
    "root_directory = os.path.join(drive, 'ACCS_Work/Projects/VegetationEcology/BristolBay_Vegetation/Project_GIS/Data_Output')\n",
    "\n",
    "# Define inputs\n",
    "subwatershed_path = os.path.join(root_directory, 'prediction_tables')\n",
    "subwatershed = os.path.join(subwatershed_path, '190303011505.csv')\n",
    "#subwatershed = os.path.join(subwatershed_path, '190303040305.csv')\n",
    "floodplain_classifier = os.path.join(root_directory, 'model_floodplain/classifier_floodplain.joblib')\n",
    "threshold_file = os.path.join(root_directory, 'model_floodplain/threshold.txt')\n",
    "\n",
    "# Define output location\n",
    "output_path = os.path.join(root_directory, 'model_floodplain/output_tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable sets\n",
    "classifier_features = ['compoundTopographic', 'elevation', 'exposure', 'heatLoad', 'moisture', 'roughness', 'slope', 'summerWarmth', 'surfaceArea', 'surfaceRelief', 'X05May_ndsi', 'X05May_ndvi', 'X05May_ndwi', 'X06June_4_red', 'X06June_ndsi', 'X06June_ndvi', 'X06June_ndwi', 'X07July_4_red', 'X07July_6_redEdge2', 'X07July_7_redEdge3', 'X07July_8_nearInfrared', 'X07July_11_shortInfrared1', 'X07July_12_shortInfrared2', 'X07July_nbr', 'X07July_ndmi', 'X07July_ndvi', 'X07July_ndwi', 'X08August_ndvi', 'X09September_1_ultraBlue', 'X09September_4_red', 'X09September_11_shortInfrared1', 'X09September_ndvi', 'X09September_ndwi', 'X10October_2_blue', 'X10October_3_green', 'X10October_4_red', 'X10October_5_redEdge1', 'X10October_6_redEdge2', 'X10October_11_shortInfrared1', 'X10October_12_shortInfrared2', 'X10October_nbr', 'X10October_ndmi', 'X10October_ndsi', 'X10October_ndvi', 'X10October_ndwi']\n",
    "coordinates = ['POINT_X', 'POINT_Y']\n",
    "predict = ['presence']\n",
    "output_variables = coordinates + predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the trained classifier\n",
    "classifier = joblib.load(floodplain_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing function execution\n",
    "start = time.time()\n",
    "# Load the input data\n",
    "input_data = pd.read_csv(subwatershed)\n",
    "total_n = len(input_data['pointid'])\n",
    "# End timing\n",
    "end = time.time()\n",
    "elapsed = int(end - start)\n",
    "success_time = datetime.datetime.now()\n",
    "# Report process success\n",
    "out_process = 'Succeeded at {0} (Elapsed time: {1})'.format(success_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                                                            datetime.timedelta(seconds=elapsed))\n",
    "print(out_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a counter\n",
    "n = 0\n",
    "# Loop through the prediction function for all input files\n",
    "while n < 10:\n",
    "    output_name = os.path.split(subwatershed)[1]\n",
    "    output_name = os.path.splitext(output_name)[0] + '-' + str(n + 1) + '.csv'\n",
    "    output_csv = os.path.join(output_path, output_name)\n",
    "    if os.path.isfile(output_csv) == False:\n",
    "        # Start timing function execution\n",
    "        start = time.time()\n",
    "        print('Predicting grid {0} out of {1}...'.format((n + 1), str(10)))\n",
    "        # Determine subset start and finish row indices\n",
    "        start = int((total_n/10) * n)\n",
    "        finish = int((total_n/10) * (n + 1)) - 1\n",
    "        # Subset data\n",
    "        subset_data = input_data.iloc[start:finish]\n",
    "        subset_data = subset_data.dropna(axis=0, how='any')\n",
    "        subset_data[classifier_features] = subset_data[classifier_features].astype(float)\n",
    "        # Define the X data\n",
    "        X_data = subset_data[classifier_features]\n",
    "        # Predict the classifier\n",
    "        classification = classifier.predict_proba(X_data)\n",
    "        # Concatenate predicted values to input data frame\n",
    "        subset_data = pd.concat([subset_data, pd.DataFrame(classification)], axis=1)\n",
    "        subset_data = subset_data.rename(index=int, columns={0: 'absence', 1: 'presence'})\n",
    "        # Export prediction to csv\n",
    "        output_data = subset_data[output_variables]\n",
    "        output_data.to_csv(output_csv, header=True, index=False, sep=',', encoding='utf-8')\n",
    "        # End timing\n",
    "        end = time.time()\n",
    "        elapsed = int(end - start)\n",
    "        success_time = datetime.datetime.now()\n",
    "        # Report process success\n",
    "        out_process = 'Succeeded at {0} (Elapsed time: {1})'.format(success_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                                                                    datetime.timedelta(seconds=elapsed))\n",
    "        print(out_process)\n",
    "        print('----------')\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
