{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Floodplain Predict\n",
    "\n",
    "**Timm Nawrocki**  \n",
    "Alaska Center for Conservation Science  \n",
    "2019-04-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------------------\n",
    "# Floodplain Predict\n",
    "# Author: Timm Nawrocki, Alaska Center for Conservation Science\n",
    "# Created on: 2019-04-20\n",
    "# Usage: Must be executed as a Jupyter Notebook in an Anaconda 3 installation.\n",
    "# Description: \"Floodplain Predict\" applies the trained classifier to data in regular point grid format stored in csv files to create a prediction representing the distribution of floodplains.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import datetime\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import time\n",
    "# Import modules for model selection, cross validation, random forest, and performance from Scikit Learn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Set root directory\n",
    "drive = 'E:/'\n",
    "root_directory = os.path.join(drive, 'ACCS_Work/Projects/VegetationEcology/BristolBay_Vegetation/Project_GIS/Data_Output')\n",
    "\n",
    "# Define inputs\n",
    "subwatershed_path = os.path.join(root_directory, 'prediction_tables')\n",
    "floodplain_classifier = os.path.join(root_directory, 'model_floodplain/classifier_floodplain.joblib')\n",
    "threshold_file = os.path.join(root_directory, 'model_floodplain/threshold.txt')\n",
    "\n",
    "# Define output location\n",
    "output_path = os.path.join(root_directory, 'model_floodplain/output_tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable sets\n",
    "classifier_features = ['compoundTopographic', 'elevation', 'exposure', 'heatLoad', 'moisture', 'roughness', 'slope', 'summerWarmth', 'surfaceArea', 'surfaceRelief', 'X05May_ndsi', 'X05May_ndvi', 'X05May_ndwi', 'X06June_4_red', 'X06June_ndsi', 'X06June_ndvi', 'X06June_ndwi', 'X07July_4_red', 'X07July_6_redEdge2', 'X07July_7_redEdge3', 'X07July_8_nearInfrared', 'X07July_11_shortInfrared1', 'X07July_12_shortInfrared2', 'X07July_nbr', 'X07July_ndmi', 'X07July_ndvi', 'X07July_ndwi', 'X08August_ndvi', 'X09September_1_ultraBlue', 'X09September_4_red', 'X09September_11_shortInfrared1', 'X09September_ndvi', 'X09September_ndwi', 'X10October_2_blue', 'X10October_3_green', 'X10October_4_red', 'X10October_5_redEdge1', 'X10October_6_redEdge2', 'X10October_11_shortInfrared1', 'X10October_12_shortInfrared2', 'X10October_nbr', 'X10October_ndmi', 'X10October_ndsi', 'X10October_ndvi', 'X10October_ndwi']\n",
    "coordinates = ['POINT_X', 'POINT_Y']\n",
    "predict = ['presence']\n",
    "output_variables = coordinates + predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read threshold values from text file\n",
    "def readThreshold(inFile):\n",
    "    threshold_reader = open(inFile, \"r\")\n",
    "    threshold = threshold_reader.readlines()\n",
    "    threshold_reader.close()\n",
    "    outThreshold = float(threshold[0])\n",
    "    return outThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the trained classifier\n",
    "classifier = joblib.load(floodplain_classifier)\n",
    "# Read thresholds from text files in the workspace folder and store as variables\n",
    "threshold = readThreshold(threshold_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 grid tiles will be processed...\n"
     ]
    }
   ],
   "source": [
    "# Define list of prediction grids\n",
    "grid_list = []\n",
    "for file in os.listdir(subwatershed_path):\n",
    "    if file.endswith('csv'):\n",
    "        grid_list.append(os.path.join(subwatershed_path, file))\n",
    "print('{0} grid tiles will be processed...'.format(len(grid_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting grid 1 out of 53...\n",
      "Succeeded at 2019-04-20 17:23 (Elapsed time: 0:09:21)\n",
      "----------\n",
      "Predicting grid 2 out of 53...\n"
     ]
    }
   ],
   "source": [
    "# Loop through the prediction function for all input files\n",
    "for grid in grid_list:\n",
    "    output_csv = os.path.join(output_path, os.path.split(grid)[1])\n",
    "    if os.path.isfile(output_csv) == False:\n",
    "        # Start timing function execution\n",
    "        start = time.time()\n",
    "        print('Predicting grid {0} out of {1}...'.format(str(grid_list.index(grid) + 1), str(len(grid_list))))\n",
    "        # Define the output csv file\n",
    "        output_csv = os.path.join(output_path, os.path.split(grid)[1])\n",
    "        # Load the input data\n",
    "        input_data = pd.read_csv(grid)\n",
    "        input_data[classifier_features] = input_data[classifier_features].astype(float)\n",
    "        # Define the X data\n",
    "        X_data = input_data[classifier_features]\n",
    "        # Predict the classifier\n",
    "        classification = classifier.predict_proba(X_data)\n",
    "        # Concatenate predicted values to input data frame\n",
    "        input_data = pd.concat([input_data, pd.DataFrame(classification)], axis=1)\n",
    "        input_data = input_data.rename(index=int, columns={0: 'absence', 1: 'presence'})\n",
    "        # Export prediction to csv\n",
    "        output_data = input_data[output_variables]\n",
    "        output_data.to_csv(output_csv, header=True, index=False, sep=',', encoding='utf-8')\n",
    "        # End timing\n",
    "        end = time.time()\n",
    "        elapsed = int(end - start)\n",
    "        success_time = datetime.datetime.now()\n",
    "        # Report process success\n",
    "        out_process = 'Succeeded at {0} (Elapsed time: {1})'.format(success_time.strftime(\"%Y-%m-%d %H:%M\"),\n",
    "                                                                    datetime.timedelta(seconds=elapsed))\n",
    "        print(out_process)\n",
    "        print('----------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
