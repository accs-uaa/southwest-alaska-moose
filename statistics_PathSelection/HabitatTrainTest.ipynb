{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Habitat Train and Test\n",
    "\n",
    "**Written by Timm Nawrocki, Amanda Droghini**\n",
    "\n",
    "*Last updated Wednesday, June 9, 2021.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------------------------------------------------------------\n",
    "# Habitat Train and Test\n",
    "# Author: Timm Nawrocki, Amanda Droghini Alaska Center for Conservation Science\n",
    "# Created on: 2021-06-09\n",
    "# Usage: Must be executed as a Jupyter Notebook in an Anaconda 3 installation.\n",
    "# Description: \"Habitat Train and Test\" trains a random forest model (i.e., path selection function) to predict habitat\n",
    "# from path covariate means. A threshold for avoidance/selection conversion is selected empirically. All model performance\n",
    "# metrics are calculated on independent test partitions.\n",
    "# ---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This script runs the model train and test steps to output a model performance and variable importance report, trained classifier file, and threshold file that can be transferred to the prediction script. The train-test classifier is set to use 4 cores. The script must be run on a machine that can support 4 cores. For information on generating inputs for this script, see the [project readme](https://github.com/accs-uaa/southwest-alaska-moose)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Data and Variables\n",
    "\n",
    "This script relies on data that has been pre-processed into a standard format csv file. The tools to generate the csv file are included in the [southwest-alaska-moose](https://github.com//accs-uaa/southwest-alaska-moose) repository. The csv file must include all summarized observed and random paths for the target species. Covariates defined below must be modified to match the input csv file if changes to the construction of covariates are made.\n",
    "\n",
    "This script has general dependencies on the *os* package for file system manipulations, the *numpy* and *pandas* packages for data manipulations, and the *seaborn* and *matplotlib* packages for plotting. *Scikit Learn* provides models, model selection, cross validation, and performance tools. Joblib provides a function to save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os\n",
    "import os\n",
    "# Import packages for file manipulation, data manipulation, and plotting\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plot\n",
    "# Import modules for model selection, cross validation, random forest, and performance from Scikit Learn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# Import joblib\n",
    "import joblib\n",
    "# Import timing packages\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set root directory\n",
    "root_folder = 'N:/ACCS_Work/Projects/WildlifeEcology/Moose_SouthwestAlaska/Data'\n",
    "# Define data folders\n",
    "data_input = os.path.join(root_folder,\n",
    "                          'Data_Input/paths')\n",
    "data_output = os.path.join(root_folder,\n",
    "                           'Data_Output/model_results/round_20210604')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variable sets\n",
    "predictor_all = ['elevation', 'roughness', 'forest_edge', 'tundra_edge', 'alnus', 'betshr',\n",
    "                 'dectre', 'dryas', 'empnig', 'erivag', 'picgla', 'picmar', 'rhoshr',\n",
    "                 'salshr', 'sphagn', 'vaculi', 'vacvit', 'wetsed']\n",
    "response = ['response']\n",
    "retain_variables = ['mooseYear_id', 'fullPath_id', 'calfStatus']\n",
    "all_variables = retain_variables + predictor_all + response\n",
    "iteration = ['iteration']\n",
    "absence = ['absence']\n",
    "presence = ['presence']\n",
    "prediction = ['prediction']\n",
    "selection = ['selection']\n",
    "output_variables = all_variables + absence + presence + prediction + selection + iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a classifier\n",
    "classifier = ExtraTreesClassifier(n_estimators = 1000,\n",
    "                                   criterion = 'gini',\n",
    "                                   max_depth = None,\n",
    "                                   min_samples_split = 2,\n",
    "                                   min_samples_leaf = 1,\n",
    "                                   min_weight_fraction_leaf = 0,\n",
    "                                   max_features = None,\n",
    "                                   bootstrap = False,\n",
    "                                   oob_score = False,\n",
    "                                   n_jobs = 2,\n",
    "                                   random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input file\n",
    "input_file = os.path.join(data_input, 'allPaths_meanCovariates_forModelRun.csv')\n",
    "# Define output folder\n",
    "output_folder = os.path.join(data_output, 'Calf_RF')\n",
    "# Define output report\n",
    "output_report_name = 'report-moose-calf.html'\n",
    "# Define species, genera, or aggregate name\n",
    "taxon_name = 'Moose with Calves'\n",
    "# Define calf status\n",
    "calf_status = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plots folder if it does not exist\n",
    "plots_folder = os.path.join(output_folder, \"plots\")\n",
    "if not os.path.exists(plots_folder):\n",
    "    os.makedirs(plots_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output test data\n",
    "output_csv = os.path.join(output_folder, 'prediction.csv')\n",
    "# Define output model file\n",
    "output_classifier = os.path.join(output_folder, 'classifier.joblib')\n",
    "# Define output threshold file\n",
    "threshold_file = os.path.join(output_folder, 'threshold.txt')\n",
    "# Define output correlation plot\n",
    "variable_correlation = os.path.join(plots_folder, \"variable_correlation.png\")\n",
    "# Define output variable importance plot\n",
    "importance_classifier = os.path.join(plots_folder, \"importance_classifier.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Functions\n",
    "\n",
    "Analyses are conducted in units represented by functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Threshold Optimization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate performance metrics based on a specified threshold value\n",
    "def testPresenceThreshold(predict_probability, threshold, y_test):\n",
    "    # Create an empty array of zeroes that matches the length of the probability predictions\n",
    "    predict_thresholded = np.zeros(predict_probability.shape)\n",
    "    # Set values for all probabilities greater than or equal to the threshold equal to 1\n",
    "    predict_thresholded[predict_probability >= threshold] = 1\n",
    "    # Determine error rates\n",
    "    confusion_test = confusion_matrix(y_test, predict_thresholded)\n",
    "    true_negative = confusion_test[0,0]\n",
    "    false_negative = confusion_test[1,0]\n",
    "    true_positive = confusion_test[1,1]\n",
    "    false_positive = confusion_test[0,1]\n",
    "    # Calculate sensitivity and specificity\n",
    "    sensitivity = true_positive / (true_positive + false_negative)\n",
    "    specificity = true_negative / (true_negative + false_positive)\n",
    "    # Calculate AUC score\n",
    "    auc = roc_auc_score(y_test, predict_probability)\n",
    "    # Calculate overall accuracy\n",
    "    accuracy = (true_negative + true_positive) / (true_negative + false_positive + false_negative + true_positive)\n",
    "    # Return the thresholded probabilities and the performance metrics\n",
    "    return (sensitivity, specificity, auc, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to determine a presence threshold\n",
    "def determineOptimalThreshold(predict_probability, y_test):\n",
    "    # Iterate through numbers between 0 and 1000 to output a list of sensitivity and specificity values per threshold number\n",
    "    i = 1\n",
    "    sensitivity_list = []\n",
    "    specificity_list = []\n",
    "    while i < 1001:\n",
    "        threshold = i/1000\n",
    "        sensitivity, specificity, auc, accuracy = testPresenceThreshold(predict_probability, threshold, y_test)\n",
    "        sensitivity_list.append(sensitivity)\n",
    "        specificity_list.append(specificity)\n",
    "        i = i + 1\n",
    "    # Calculate a list of absolute value difference between sensitivity and specificity and find the optimal threshold\n",
    "    difference_list = [np.absolute(a - b) for a, b in zip(sensitivity_list, specificity_list)]\n",
    "    value, threshold = min((value, threshold) for (threshold, value) in enumerate(difference_list))\n",
    "    threshold = threshold/1000\n",
    "    # Calculate the performance of the optimal threshold\n",
    "    sensitivity, specificity, auc, accuracy = testPresenceThreshold(predict_probability, threshold, y_test)\n",
    "    # Return the optimal threshold and the performance metrics of the optimal threshold\n",
    "    return threshold, sensitivity, specificity, auc, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Export Results Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to composite model results\n",
    "def compositeSelection(input_data, presence, threshold, output):\n",
    "    # Determine positive and negative ranges\n",
    "    positive_range = input_data[presence[0]].max() - threshold\n",
    "    negative_range = threshold - input_data[presence[0]].min()\n",
    "    # Define a function to threshold presences and absences and standardize values from -1 (avoidance) to 1 (selection)\n",
    "    def compositeRows(row):\n",
    "        if row[presence[0]] == threshold:\n",
    "            return 0\n",
    "        elif row[presence[0]] > threshold:\n",
    "            adjusted_value = (row[presence[0]] - threshold) / positive_range\n",
    "            return adjusted_value\n",
    "        elif row[presence[0]] < threshold:\n",
    "            adjusted_value = (row[presence[0]] - threshold) / negative_range\n",
    "            return adjusted_value\n",
    "    # Apply function to all rows in data\n",
    "    input_data[output[0]] = input_data.apply(lambda row: compositeRows(row), axis=1)\n",
    "    # Return the data frame with composited results\n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot Pearson correlation of predictor variables\n",
    "def plotVariableCorrelation(X_train, outFile):\n",
    "    # Calculate Pearson correlation coefficient between the predictor variables, where -1 is perfect negative correlation and 1 is perfect positive correlation\n",
    "    correlation = X_train.astype('float64').corr()\n",
    "    # Generate a mask for the upper triangle of plot\n",
    "    mask = np.zeros_like(correlation, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plot.subplots(figsize=(10, 9))\n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    correlation_plot = sns.heatmap(correlation, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={'shrink': .5})\n",
    "    correlation_figure = correlation_plot.get_figure()\n",
    "    correlation_figure.savefig(outFile, bbox_inches='tight', dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to plot variable importances\n",
    "def plotVariableImportances(inModel, X_train, outVariableFile):\n",
    "    # Get numerical feature importances\n",
    "    importances = list(inModel.feature_importances_)\n",
    "    # List of tuples with variable and importance\n",
    "    feature_list = list(X_train.columns)\n",
    "    feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list, importances)]\n",
    "    # Sort the feature importances by most important first\n",
    "    feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "    # Initialize the plot and set figure size\n",
    "    variable_figure = plot.figure()\n",
    "    fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "    fig_size[0] = 20\n",
    "    fig_size[1] = 6\n",
    "    plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "    # Create list of x locations for plotting\n",
    "    x_values = list(range(len(importances)))\n",
    "    # Make a bar chart of the variable importances\n",
    "    plot.bar(x_values, importances, orientation = 'vertical')\n",
    "    # Tick labels for x axis\n",
    "    plot.xticks(x_values, feature_list, rotation='vertical')\n",
    "    # Axis labels and title\n",
    "    plot.ylabel('Importance'); plot.xlabel('Variable'); plot.title('Variable Importances');\n",
    "    # Export\n",
    "    variable_figure.savefig(outVariableFile, bbox_inches=\"tight\", dpi=300)\n",
    "    # Clear plot workspace\n",
    "    plot.clf()\n",
    "    plot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conduct Analyses\n",
    "\n",
    "The analyses are subdivided into subsections: load data, train and test iterations with nested cross validation, final model training, and export results. Nested cross-validation is necessary to maintain the independence of the test data outside of cross-validated threshold optimization. The prediction results of the outer cross-validation, wherein each sample is predicted a single time, are appended into a single data frame for additional analyses and plotting external to this script.\n",
    "\n",
    "Outputs of the analysis are:\n",
    "1. Report of performance, including the following plots:\n",
    "  1. Final selection of the classifier hyperparameters\n",
    "  2. Pearson correlation for all predictors\n",
    "2. Model files:\n",
    "  1. Final Classifier\n",
    "3. Report with overall AUC and accuracy.\n",
    "4. Final threshold\n",
    "5. Test predictions from 5-fold cross-validation (each sample predicted once)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame of input data\n",
    "data_all = pd.read_csv(input_file)\n",
    "input_data = data_all[data_all['calfStatus'] == calf_status].copy()\n",
    "# Rename covariates to match prediction grids\n",
    "input_data = input_data.rename(columns={'elevation_mean': 'elevation',\n",
    "                                        'roughness_mean': 'roughness',\n",
    "                                        'forest_edge_mean': 'forest_edge',\n",
    "                                        'tundra_edge_mean': 'tundra_edge',\n",
    "                                        'alnus_mean': 'alnus',\n",
    "                                        'betshr_mean': 'betshr',\n",
    "                                        'dectre_mean': 'dectre',\n",
    "                                        'dryas_mean': 'dryas',\n",
    "                                        'empnig_mean': 'empnig',\n",
    "                                        'erivag_mean': 'erivag',\n",
    "                                        'picgla_mean': 'picgla',\n",
    "                                        'picmar_mean': 'picmar',\n",
    "                                        'rhoshr_mean': 'rhoshr',\n",
    "                                        'salshr_mean': 'salshr',\n",
    "                                        'sphagn_mean': 'sphagn',\n",
    "                                        'vaculi_mean': 'vaculi',\n",
    "                                        'vacvit_mean': 'vacvit',\n",
    "                                        'wetsed_mean': 'wetsed'})\n",
    "# Shuffle data\n",
    "input_data = shuffle(input_data, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the X and y data for classification\n",
    "X_classify = input_data[predictor_all].astype(float)\n",
    "y_classify = input_data[response[0]].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 8\n",
    "fig_size[1] = 6\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size\n",
    "plot.style.use('grayscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Train and Test Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 5-fold cross validation split methods\n",
    "outer_cv_splits = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 5)\n",
    "inner_cv_splits = StratifiedKFold(n_splits = 5, shuffle = False, random_state = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store threshold and performance metrics\n",
    "threshold_list = []\n",
    "# Create an empty data frame to store the outer cross validation splits\n",
    "outer_train = pd.DataFrame(columns = all_variables + iteration)\n",
    "outer_test = pd.DataFrame(columns = all_variables + iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty data frame to store the outer test results\n",
    "outer_results = pd.DataFrame(columns = output_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outer cross validation splits for cover data\n",
    "count = 1\n",
    "for train_index, test_index in outer_cv_splits.split(input_data, input_data[response[0]]):\n",
    "    # Split the data into train and test partitions\n",
    "    train = input_data.iloc[train_index]\n",
    "    test = input_data.iloc[test_index]\n",
    "    # Insert iteration to train\n",
    "    train = train.assign(iteration = count)\n",
    "    # Insert iteration to test\n",
    "    test = test.assign(iteration = count)\n",
    "    # Append to data frames\n",
    "    outer_train = outer_train.append(train, ignore_index = True, sort = True)\n",
    "    outer_test = outer_test.append(test, ignore_index = True, sort = True)\n",
    "    # Increase counter\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset indices\n",
    "outer_train = outer_train.reset_index()\n",
    "outer_test = outer_test.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conducting outer cross-validation iteration 1 of 5...\n",
      "\tOptimizing classification threshold...\n",
      "\tCompleted at 2021-06-09 10:34 (Elapsed time: 0:00:06)\n",
      "\t----------\n",
      "\tTraining classifier...\n",
      "\tCompleted at 2021-06-09 10:34 (Elapsed time: 0:00:00)\n",
      "\t----------\n",
      "\tPredicting outer cross-validation test data...\n",
      "\tCompleted at 2021-06-09 10:34 (Elapsed time: 0:00:00)\n",
      "\t----------\n",
      "Conducting outer cross-validation iteration 2 of 5...\n",
      "\tOptimizing classification threshold...\n",
      "\tCompleted at 2021-06-09 10:34 (Elapsed time: 0:00:06)\n",
      "\t----------\n",
      "\tTraining classifier...\n",
      "\tCompleted at 2021-06-09 10:34 (Elapsed time: 0:00:00)\n",
      "\t----------\n",
      "\tPredicting outer cross-validation test data...\n",
      "\tCompleted at 2021-06-09 10:34 (Elapsed time: 0:00:00)\n",
      "\t----------\n",
      "Conducting outer cross-validation iteration 3 of 5...\n",
      "\tOptimizing classification threshold...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:06)\n",
      "\t----------\n",
      "\tTraining classifier...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:01)\n",
      "\t----------\n",
      "\tPredicting outer cross-validation test data...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:00)\n",
      "\t----------\n",
      "Conducting outer cross-validation iteration 4 of 5...\n",
      "\tOptimizing classification threshold...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:06)\n",
      "\t----------\n",
      "\tTraining classifier...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:00)\n",
      "\t----------\n",
      "\tPredicting outer cross-validation test data...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:00)\n",
      "\t----------\n",
      "Conducting outer cross-validation iteration 5 of 5...\n",
      "\tOptimizing classification threshold...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:06)\n",
      "\t----------\n",
      "\tTraining classifier...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:00)\n",
      "\t----------\n",
      "\tPredicting outer cross-validation test data...\n",
      "\tCompleted at 2021-06-09 10:35 (Elapsed time: 0:00:00)\n",
      "\t----------\n"
     ]
    }
   ],
   "source": [
    "#### MODEL TRAIN AND TEST ITERATIONS WITH HYPERPARAMETER AND THRESHOLD OPTIMIZATION IN NESTED CROSS-VALIDATION\n",
    "####____________________________________________________\n",
    "\n",
    "# Iterate through outer cross validation splits\n",
    "i = 1\n",
    "while i < 6:\n",
    "    \n",
    "    \n",
    "    #### CONDUCT MODEL TRAIN\n",
    "    ####____________________________________________________\n",
    "    \n",
    "    # Partition the outer train split by iteration number\n",
    "    print(f'Conducting outer cross-validation iteration {i} of 5...')\n",
    "    train_iteration = outer_train[outer_train[iteration[0]] == i]\n",
    "    \n",
    "    # Identify X and y train splits for the classifier\n",
    "    X_train_classify = train_iteration[predictor_all].astype(float)\n",
    "    y_train_classify = train_iteration[response[0]].astype('int32')\n",
    "    \n",
    "    # Predict each training data row in inner cross validation\n",
    "    print('\\tOptimizing classification threshold...')\n",
    "    iteration_start = time.time()\n",
    "          \n",
    "    # Create an empty data frame to store the inner cross validation splits\n",
    "    inner_train = pd.DataFrame(columns = all_variables + iteration + ['inner'])\n",
    "    inner_test = pd.DataFrame(columns = all_variables + iteration + ['inner'])\n",
    "          \n",
    "    # Create an empty data frame to store the inner test results\n",
    "    inner_results = pd.DataFrame(columns = all_variables + absence + presence + prediction + iteration + ['inner'])\n",
    "          \n",
    "    # Create inner cross validation splits\n",
    "    count = 1\n",
    "    for train_index, test_index in inner_cv_splits.split(train_iteration, train_iteration[response[0]].astype('int32')):\n",
    "        # Split the data into train and test partitions\n",
    "        train = train_iteration.iloc[train_index]\n",
    "        test = train_iteration.iloc[test_index]\n",
    "        # Insert iteration to train\n",
    "        train = train.assign(inner = count)\n",
    "        # Insert iteration to test\n",
    "        test = test.assign(inner = count)\n",
    "        # Append to data frames\n",
    "        inner_train = inner_train.append(train, ignore_index=True, sort=True)\n",
    "        inner_test = inner_test.append(test, ignore_index=True, sort=True)\n",
    "        # Increase counter\n",
    "        count += 1\n",
    "          \n",
    "    # Iterate through inner cross validation splits\n",
    "    n = 1\n",
    "    while n < 6:\n",
    "        inner_train_iteration = inner_train[inner_train['inner'] == n]\n",
    "        inner_test_iteration = inner_test[inner_test['inner'] == n]\n",
    "    \n",
    "        # Identify X and y inner train and test splits\n",
    "        X_train_inner = inner_train_iteration[predictor_all].astype(float)\n",
    "        y_train_inner = inner_train_iteration[response[0]].astype('int32')\n",
    "        X_test_inner = inner_test_iteration[predictor_all].astype(float)\n",
    "        y_test_inner = inner_test_iteration[response[0]].astype('int32')\n",
    "        \n",
    "        # Train classifier on the inner train data\n",
    "        classifier.fit(X_train_inner, y_train_inner)\n",
    "        \n",
    "        # Predict probabilities for inner test data\n",
    "        probability_inner = classifier.predict_proba(X_test_inner)\n",
    "        # Concatenate predicted values to test data frame\n",
    "        inner_test_iteration = inner_test_iteration.assign(absence = probability_inner[:,0])\n",
    "        inner_test_iteration = inner_test_iteration.assign(presence = probability_inner[:,1])\n",
    "          \n",
    "        # Add iteration number to inner test iteration\n",
    "        inner_test_iteration = inner_test_iteration.assign(inner = n)\n",
    "    \n",
    "        # Add the test results to output data frame\n",
    "        inner_results = inner_results.append(inner_test_iteration, ignore_index=True, sort=True)\n",
    "        \n",
    "        # Increase n value\n",
    "        n += 1\n",
    "    \n",
    "    # Calculate the optimal threshold and performance of the presence-absence classification\n",
    "    inner_results[response[0]] = inner_results[response[0]].astype('int32')\n",
    "    threshold, sensitivity, specificity, auc, accuracy = determineOptimalThreshold(inner_results[presence[0]],\n",
    "                                                                                   inner_results[response[0]])\n",
    "    threshold_list.append(threshold)\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "    \n",
    "    # Train classifier\n",
    "    print('\\tTraining classifier...')\n",
    "    iteration_start = time.time()\n",
    "    classifier.fit(X_train_classify, y_train_classify)\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "          \n",
    "    #### CONDUCT MODEL TEST\n",
    "    ####____________________________________________________\n",
    "    \n",
    "    # Partition the outer test split by iteration number\n",
    "    print('\\tPredicting outer cross-validation test data...')\n",
    "    iteration_start = time.time()\n",
    "    test_iteration = outer_test[outer_test[iteration[0]] == i]\n",
    "    \n",
    "    # Identify X test split\n",
    "    X_test = test_iteration[predictor_all]\n",
    "    \n",
    "    # Use the classifier to predict class probabilities\n",
    "    probability_prediction = classifier.predict_proba(X_test)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    test_iteration = test_iteration.assign(absence = probability_prediction[:,0])\n",
    "    test_iteration = test_iteration.assign(presence = probability_prediction[:,1])\n",
    "    \n",
    "    # Convert probability to presence-absence\n",
    "    presence_zeros = np.zeros(test_iteration[presence[0]].shape)\n",
    "    presence_zeros[test_iteration[presence[0]] >= threshold] = 1\n",
    "    # Concatenate distribution values to test data frame\n",
    "    test_iteration = test_iteration.assign(prediction = presence_zeros)\n",
    "    \n",
    "    # Convert probability to selection\n",
    "    test_iteration = compositeSelection(test_iteration, presence, threshold, selection)\n",
    "    \n",
    "    # Add iteration number to test iteration\n",
    "    test_iteration = test_iteration.assign(iteration = i)\n",
    "    \n",
    "    # Add the test results to output data frame\n",
    "    outer_results = outer_results.append(test_iteration, ignore_index=True, sort=True)\n",
    "    iteration_end = time.time()\n",
    "    iteration_elapsed = int(iteration_end - iteration_start)\n",
    "    iteration_success_time = datetime.datetime.now()\n",
    "    print(f'\\tCompleted at {iteration_success_time.strftime(\"%Y-%m-%d %H:%M\")} (Elapsed time: {datetime.timedelta(seconds=iteration_elapsed)})')\n",
    "    print('\\t----------')\n",
    "    \n",
    "    # Increase iteration number\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539\n"
     ]
    }
   ],
   "source": [
    "print(len(outer_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final AUC = 0.8386505622657225\n",
      "Final Accuracy = 0.75139146567718\n"
     ]
    }
   ],
   "source": [
    "# Partition output results to presence-absence observed and predicted\n",
    "y_classify_observed = outer_results[response[0]].astype('int32')\n",
    "y_classify_predicted = outer_results[prediction[0]].astype('int32')\n",
    "y_classify_probability = outer_results[presence[0]]\n",
    "\n",
    "# Determine error rates\n",
    "confusion_test = confusion_matrix(y_classify_observed, y_classify_predicted)\n",
    "true_negative = confusion_test[0,0]\n",
    "false_negative = confusion_test[1,0]\n",
    "true_positive = confusion_test[1,1]\n",
    "false_positive = confusion_test[0,1]\n",
    "# Calculate sensitivity and specificity\n",
    "sensitivity = true_positive / (true_positive + false_negative)\n",
    "specificity = true_negative / (true_negative + false_positive)\n",
    "# Calculate AUC score\n",
    "fin_auc = roc_auc_score(y_classify_observed, y_classify_probability)\n",
    "# Calculate overall accuracy\n",
    "fin_accuracy = (true_negative + true_positive) / (true_negative + false_positive + false_negative + true_positive)\n",
    "\n",
    "# Print performance results\n",
    "print(f'Final AUC = {fin_auc}')\n",
    "print(f'Final Accuracy = {fin_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Train Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 12\n",
    "fig_size[1] = 4\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TRAIN AND EXPORT FINAL CLASSIFIER\n",
    "\n",
    "# Create an empty data frame to store the inner cross validation splits\n",
    "inner_train = pd.DataFrame(columns = all_variables + iteration + ['inner'])\n",
    "inner_test = pd.DataFrame(columns = all_variables + iteration + ['inner'])\n",
    "          \n",
    "# Create an empty data frame to store the inner test results\n",
    "inner_results = pd.DataFrame(columns = all_variables + absence + presence + prediction + iteration + ['inner'])\n",
    "          \n",
    "# Create inner cross validation splits for input data\n",
    "count = 1\n",
    "for train_index, test_index in inner_cv_splits.split(input_data, input_data[response[0]].astype('int32')):\n",
    "    # Split the data into train and test partitions\n",
    "    train = input_data.iloc[train_index]\n",
    "    test = input_data.iloc[test_index]\n",
    "    # Insert iteration to train\n",
    "    train = train.assign(inner = count)\n",
    "    # Insert iteration to test\n",
    "    test = test.assign(inner = count)\n",
    "    # Append to data frames\n",
    "    inner_train = inner_train.append(train, ignore_index=True, sort=True)\n",
    "    inner_test = inner_test.append(test, ignore_index=True, sort=True)\n",
    "    # Increase counter\n",
    "    count += 1\n",
    "          \n",
    "# Iterate through inner cross validation splits\n",
    "n = 1\n",
    "while n < 6:\n",
    "    inner_train_iteration = inner_train[inner_train['inner'] == n]\n",
    "    inner_test_iteration = inner_test[inner_test['inner'] == n]\n",
    "    \n",
    "    # Identify X and y inner train and test splits\n",
    "    X_train_inner = inner_train_iteration[predictor_all].astype(float)\n",
    "    y_train_inner = inner_train_iteration[response[0]].astype('int32')\n",
    "    X_test_inner = inner_test_iteration[predictor_all].astype(float)\n",
    "    y_test_inner = inner_test_iteration[response[0]].astype('int32')\n",
    "        \n",
    "    # Train classifier on the inner train data\n",
    "    classifier.fit(X_train_inner, y_train_inner)\n",
    "        \n",
    "    # Predict probabilities for inner test data\n",
    "    probability_inner = classifier.predict_proba(X_test_inner)\n",
    "    # Concatenate predicted values to test data frame\n",
    "    inner_test_iteration = inner_test_iteration.assign(absence = probability_inner[:,0])\n",
    "    inner_test_iteration = inner_test_iteration.assign(presence = probability_inner[:,1])\n",
    "          \n",
    "    # Add iteration number to inner test iteration\n",
    "    inner_test_iteration = inner_test_iteration.assign(inner = n)\n",
    "    \n",
    "    # Add the test results to output data frame\n",
    "    inner_results = inner_results.append(inner_test_iteration, ignore_index=True, sort=True)\n",
    "    \n",
    "    # Increase n value\n",
    "    n += 1\n",
    "    \n",
    "# Calculate the optimal threshold and performance of the presence-absence classification\n",
    "inner_results[response[0]] = inner_results[response[0]].astype('int32')\n",
    "threshold_final, sensitivity, specificity, auc, accuracy = determineOptimalThreshold(inner_results[presence[0]], inner_results[response[0]])\n",
    "\n",
    "# Write a text file to store the presence-absence conversion threshold\n",
    "file = open(threshold_file, 'w')\n",
    "file.write(str(round(threshold_final, 5)))\n",
    "file.close()\n",
    "    \n",
    "# Train classifier\n",
    "classifier.fit(X_classify, y_classify)\n",
    "\n",
    "# Save classifier to an external file\n",
    "joblib.dump(classifier, output_classifier)\n",
    "# Export a variable importance plot\n",
    "plotVariableImportances(classifier, X_classify, importance_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8423157017909204\n",
      "0.7755102040816326\n"
     ]
    }
   ],
   "source": [
    "print(auc)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539\n"
     ]
    }
   ],
   "source": [
    "print(len(inner_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export test results to csv\n",
    "outer_results.to_csv(output_csv, header=True, index=False, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial plot sizefig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size = plot.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 8\n",
    "fig_size[1] = 6\n",
    "plot.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-74624d887f02>:6: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask = np.zeros_like(correlation, dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "# Export a Pearson Correlation plot for the predictor variables\n",
    "plotVariableCorrelation(input_data[predictor_all], variable_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write html text file\n",
    "output_report = os.path.join(output_folder, output_report_name)\n",
    "output_text = os.path.splitext(output_report)[0] + \".txt\"\n",
    "text_file = open(output_text, \"w\")\n",
    "text_file.write(f\"<html>\\n\")\n",
    "text_file.write(f\"<head>\\n\")\n",
    "text_file.write(f\"<meta http-equiv=\\\"pragma\\\" content=\\\"no-cache\\\">\\n\")\n",
    "text_file.write(f\"<meta http-equiv=\\\"Expires\\\" content=\\\"-1\\\">\\n\")\n",
    "text_file.write(f\"</head>\\n\")\n",
    "text_file.write(f\"<body>\\n\")\n",
    "text_file.write(f\"<div style=\\\"width:90%;max-width:1000px;margin-left:auto;margin-right:auto\\\">\\n\")\n",
    "text_file.write(f\"<h1 style=\\\"text-align:center;\\\">Distribution model performance for \" + taxon_name + \"</h1>\\n\")\n",
    "text_file.write(f\"<br>\" + \"\\n\")\n",
    "text_file.write(f\"<h2>Predicted Distribution-abundance Pattern</h2>\\n\")\n",
    "text_file.write(f\"<p>Distribution-abundance was predicted by a composite hierarchical model: 1. a classifier predicted species presence or absence, and 2. a regressor predicted species foliar cover in areas where the classifier predicted the species to be present. The map below shows the output raster prediction.</p>\\n\")\n",
    "text_file.write(f\"<p><i>Prediction step has not yet been performed. No output to display.</i></p>\\n\")\n",
    "text_file.write(f\"<h2>Bayesian Optimization of Hyperparameters</h2>\\n\")\n",
    "text_file.write(f\"<p>The hyperparameters of the classifier and regressor were independently optimized in a bayesian framework using a Gaussian Process as the generative model. Optimization performance was determined by 5-fold cross validation using area under the receiver operating characteristic curve (AUC) as the metric to maximize for the classifier and R squared as the metric to maximize for the regressor. 250 optimization iterations were performed for the classifier and 500 were performed for the regressor. The best set of parameters was selected based on the maximization criteria.</p>\\n\")\n",
    "text_file.write(f\"<h2>Composite Model Performance</h2>\\n\")\n",
    "text_file.write(f\"<p>Model performance was measured by calculating R squared, mean absolute error, and root mean squared error for the composite prediction across five independent cross validation folds. Within each cross validation fold, the Bayesian Optimization and threshold calculation were nested with 5-fold cross validation. Nested cross-validation maintained the independence of the test partition through multiple rounds of cross-validated model optimization. Additionally, the performance of the absence class is reported as an area under the receiver operating characteristic curve (AUC) and overall accuracy, where specificity and sensitivity are as close to equal as possible (i.e, the model performs equally well at predicting absences and presences). All performance results are reported from the merged results of all cross-validation runs (i.e., the performance is based on inclusion of each sample in the test set once).</p>\\n\")\n",
    "text_file.write(f\"<h3>Overall Performance</h3>\\n\")\n",
    "text_file.write(f\"<p>AUC = \" + str(np.round(fin_auc, 2)) + \"</p>\\n\")\n",
    "text_file.write(f\"<p>Presence-Absence Accuracy = \" + str(np.round(fin_accuracy, 2)) + \"</p>\\n\")\n",
    "text_file.write(f\"<h3>Classifier Importances</h3>\\n\")\n",
    "text_file.write(f\"<p>The Variable Importance plot for the classifier is shown below:</p>\\n\")\n",
    "text_file.write(f\"<a target='_blank' href='plots\\\\importance_classifier.png'><img style='display:inline-block;max-width:1000px;width:100%;' src='plots\\\\importance_classifier.png'></a>\\n\")\n",
    "text_file.write(fr\"<h2>Variable Correlation</h2>\" + \"\\n\")\n",
    "text_file.write(f\"<p>The plot below explores variable correlation. No attempt was made to remove highly correlated variables (shown in the plot dark blue).</p>\\n\")\n",
    "text_file.write(f\"<a target='_blank' href='plots\\\\variable_correlation.png'><img style='display:inline-block;width:100%;' src='plots\\\\variable_correlation.png'></a>\\n\")\n",
    "text_file.write(f\"</div>\\n\")\n",
    "text_file.write(f\"</body>\\n\")\n",
    "text_file.write(f\"</html>\\n\")\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename HTML Text to HTML\n",
    "if os.path.exists(output_report) == True:\n",
    "    os.remove(output_report)\n",
    "os.rename(output_text, output_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
